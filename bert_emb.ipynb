{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import callbacks\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model_name = 'pretrained-mlp'\n",
    "dense_layer_sizes = [37]\n",
    "dropout_rate = 0.6\n",
    "learning_rate = 0.001\n",
    "n_fold = 5\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "patience = 100\n",
    "# n_test = 100\n",
    "lambd = 0.1  # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    X = layers.Dense(dense_layer_sizes[0], name='dense0')(X_input)\n",
    "    X = layers.BatchNormalization(name='bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed=7)(X)\n",
    "\n",
    "    # Second dense layer\n",
    "    # X = layers.Dense(dense_layer_sizes[0], name = 'dense1')(X)\n",
    "    # X = layers.BatchNormalization(name = 'bn1')(X)\n",
    "    # X = layers.Activation('relu')(X)\n",
    "    # X = layers.Dropout(dropout_rate, seed = 9)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name='output', kernel_regularizer=regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input=X_input, output=X, name='classif_model')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(embeddings):\n",
    "    \"\"\"\n",
    "    Parses the embeddings given by BERT, and suitably formats them to be passed to the MLP model\n",
    "\n",
    "    Input: embeddings (DataFrame) containing contextual embeddings from BERT, also labels for the classification problem\n",
    "    columns: \"emb_A\": contextual embedding for the word A\n",
    "             \"emb_B\": contextual embedding for the word B\n",
    "             \"emb_P\": contextual embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\n",
    "    Output: X (numpy array) for each line in the GAP file, the concatenation of the embeddings of the target words\n",
    "            Y (numpy array) for each line in the GAP file, the one-hot encoded answer to the coreference problem\n",
    "    \"\"\"\n",
    "    embeddings.sort_index(\n",
    "        inplace=True)  # Sorting the DataFrame, because reading from the json file messed with the order\n",
    "    N = len(embeddings)\n",
    "    dims = 1024\n",
    "    Y = np.zeros((N, 3))\n",
    "    all_P = np.zeros((N, dims))\n",
    "    all_A = np.zeros((N, dims))\n",
    "    all_B = np.zeros((N, dims))\n",
    "        # Concatenate features\n",
    "    for i in range(len(embeddings)):\n",
    "        all_A[i] = np.array(embeddings.loc[i, 'emb_A'])\n",
    "        all_B[i] = np.array(embeddings.loc[i, 'emb_B'])\n",
    "        all_P[i] = np.array(embeddings.loc[i, 'emb_P'])\n",
    "#         all_A.append(np.array(embeddings.loc[i, 'emb_A']))\n",
    "#         all_B.append(np.array(embeddings.loc[i, 'emb_B']))\n",
    "#         all_P.append(np.array(embeddings.loc[i, 'emb_P']))\n",
    "\n",
    "    # One-hot encoding for labels\n",
    "    for i in range(len(embeddings)):\n",
    "        label = embeddings.loc[i, 'label']\n",
    "        if label == 'A':\n",
    "            Y[i, 0] = 1\n",
    "        elif label == 'B':\n",
    "            Y[i, 1] = 1\n",
    "        else:\n",
    "            Y[i, 2] = 1\n",
    "#     all_P = np.array(all_P, dtype=np.float32)\n",
    "#     all_A = np.array(all_A, dtype=np.float32)\n",
    "#     all_B = np.array(all_B, dtype=np.float32)\n",
    "    \n",
    "#     np.nan_to_num(all_P, copy=False)\n",
    "#     np.nan_to_num(all_A, copy=False)\n",
    "#     np.nan_to_num(all_B, copy=False)\n",
    "    return [all_A, all_B, all_P], Y\n",
    "\n",
    "OOF_NAME, DATA = 'nn_base_emb_gap', 'large-emb-gap'\n",
    "\n",
    "development = pd.read_json('data/%s-development.json' % DATA)\n",
    "validation = pd.read_json('data/%s-validation.json'% DATA)\n",
    "test = pd.read_json('data/%s-test.json'% DATA)\n",
    "\n",
    "new_train = pd.concat([validation, test])\n",
    "new_train = new_train.reset_index(drop=True)\n",
    "\n",
    "X_train, Y_train = parse_json(new_train)\n",
    "\n",
    "X_test, Y_test = parse_json(development)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(X_test[0][208]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(development)):\n",
    "    if len(development.loc[0, 'emb_A']) != 768:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(embeddings):\n",
    "    \"\"\"\n",
    "    Parses the embeddings given by BERT, and suitably formats them to be passed to the MLP model\n",
    "\n",
    "    Input: embeddings (DataFrame) containing contextual embeddings from BERT, also labels for the classification problem\n",
    "    columns: \"emb_A\": contextual embedding for the word A\n",
    "             \"emb_B\": contextual embedding for the word B\n",
    "             \"emb_P\": contextual embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\n",
    "    Output: X (numpy array) for each line in the GAP file, the concatenation of the embeddings of the target words\n",
    "            Y (numpy array) for each line in the GAP file, the one-hot encoded answer to the coreference problem\n",
    "    \"\"\"\n",
    "    embeddings.sort_index(\n",
    "        inplace=True)  # Sorting the DataFrame, because reading from the json file messed with the order\n",
    "    X = np.zeros((len(embeddings), 3 * 1024))\n",
    "    Y = np.zeros((len(embeddings), 3))\n",
    "\n",
    "    # Concatenate features\n",
    "    for i in range(len(embeddings)):\n",
    "        A = np.array(embeddings.loc[i, 'emb_A'])\n",
    "        B = np.array(embeddings.loc[i, 'emb_B'])\n",
    "        P = np.array(embeddings.loc[i, 'emb_P'])\n",
    "        X[i] = np.concatenate((A, B, P))\n",
    "\n",
    "#     np.nan_to_num(X, copy=False)\n",
    "    # One-hot encoding for labels\n",
    "    for i in range(len(embeddings)):\n",
    "        label = embeddings.loc[i, 'label']\n",
    "        if label == 'A':\n",
    "            Y[i, 0] = 1\n",
    "        elif label == 'B':\n",
    "            Y[i, 1] = 1\n",
    "        else:\n",
    "            Y[i, 2] = 1\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Read development embeddings from json file - this is the output of Bert\n",
    "development = pd.read_json('data/large-emb-gap-development.json')\n",
    "X_development, Y_development = parse_json(development)\n",
    "\n",
    "validation = pd.read_json('data/large-emb-gap-validation.json')\n",
    "X_validation, Y_validation = parse_json(validation)\n",
    "\n",
    "test = pd.read_json('data/large-emb-gap-test.json')\n",
    "X_test, Y_test = parse_json(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[209, 1988]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_test = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row]))]\n",
    "remove_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[209, 1988]\n"
     ]
    }
   ],
   "source": [
    "remove_test = [row for row in range(len(X_test[0])) if np.sum(np.isnan(X_test[0][row]))]\n",
    "print(remove_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2454, 1024)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.738879, -0.22716499999999998, 0.1093045, -0.5271255, -0.1011425,\n",
       "       -0.3374955, 1.443006, -0.11599899999999999, -0.1925945, 0.099926,\n",
       "       0.0966345, -0.335821, 0.5851005, -0.24083549999999998, -0.342422,\n",
       "       0.47573099999999996, -0.1167145, 0.399569, -0.20975449999999998,\n",
       "       0.033386, 0.5201835, 0.143432, -0.6489345, 0.31892149999999997,\n",
       "       0.09772499999999999, 0.48163599999999995, -0.1387115, 0.4075855,\n",
       "       -0.0728805, 0.99753, 0.747265, -0.019926, 0.5298205,\n",
       "       0.23479049999999999, 0.35161149999999997, 0.41965149999999996,\n",
       "       -0.265684, -0.27146349999999997, 0.057056499999999996, -0.6913,\n",
       "       0.1341615, -0.484091, 0.13802899999999999, -0.1446525, -0.147395,\n",
       "       -0.107264, 0.9926904999999999, -0.493407, 0.5483445,\n",
       "       -0.40435099999999996, -1.058451, 0.2930125, -0.215253, -0.502663,\n",
       "       0.312512, -0.275433, -0.009658, -0.8013205, -0.568321, -0.4131915,\n",
       "       0.3421835, 0.0686625, -0.7094765, -0.5369245,\n",
       "       -0.042256999999999996, 0.41067899999999996, 0.756239, 0.6370625,\n",
       "       0.2223415, -0.3919205, 0.040175999999999996, -0.342155, 0.0418995,\n",
       "       0.1538185, -0.359587, 0.7086604999999999, 0.2124535,\n",
       "       0.20446799999999998, -0.172845, 0.8478485, -0.3214065, 0.7436505,\n",
       "       0.33849399999999996, 1.022316, 0.700975, 0.1208745, -0.8138255,\n",
       "       0.801163, -0.740452, 1.109417, -0.3201345, 0.006770000000000001,\n",
       "       -0.002484, -0.006849, 0.14913099999999999, -0.298377, 0.52442,\n",
       "       0.317414, 0.287361, -1.0063655, -1.090529, -0.2089105, -0.2666095,\n",
       "       -0.3366285, -0.809635, -0.5585445, -0.3989025, -0.7373135,\n",
       "       -0.358543, 0.2374325, -0.4991825, -0.6225339999999999, 0.0513065,\n",
       "       0.10284499999999999, -0.022188, 0.6732815, 0.052308, -0.120533,\n",
       "       -0.49852949999999996, 0.3155555, 0.27962149999999997, 0.026306,\n",
       "       -0.212722, 1.002108, -0.101979, -0.13884449999999998,\n",
       "       0.5254624999999999, -0.0453875, -0.4438165, -0.4267315, -0.1544295,\n",
       "       0.210439, -0.5078509999999999, 0.012986999999999999, -0.911692,\n",
       "       0.149953, -0.575863, 0.3396055, 0.186502, 0.6584105,\n",
       "       0.23446000000000003, 0.2181485, 0.402501, 0.7405815,\n",
       "       0.23183099999999998, -0.09442, 0.144005, -0.3571785, -0.466895,\n",
       "       0.195155, -0.106942, 0.17460599999999998, 0.256674,\n",
       "       0.23260499999999998, -0.058413, 0.23927, 0.7893475, 0.252124,\n",
       "       -0.581961, -0.4591645, -0.0970905, -0.5652895,\n",
       "       -0.45306949999999996, 0.307735, 0.711764, -0.053686, 0.0085825,\n",
       "       -0.053982499999999996, -0.825809, -0.40512549999999997,\n",
       "       0.06432349999999999, 0.2511675, -0.38346549999999996,\n",
       "       0.6561859999999999, -0.0265835, 0.032356499999999996, 0.3220125,\n",
       "       0.7716295, 0.1873205, 0.478479, -0.19880899999999999, -0.045769,\n",
       "       -0.5545114999999999, 0.047624, 0.9362334999999999, 0.455092,\n",
       "       -0.173792, 0.015488499999999999, -0.1611415, 0.3402215, -0.9353075,\n",
       "       -0.9613765, 0.39366199999999996, 0.24812399999999998, 0.1493405,\n",
       "       -0.102417, 0.6247145, 0.022376999999999998, 0.38767199999999996,\n",
       "       0.1444945, -0.16408399999999998, 0.3147715, -0.9036649999999999,\n",
       "       0.2774065, -0.401499, 0.1752985, 0.988707, 0.07109549999999999,\n",
       "       -0.088769, 0.348192, -0.25129599999999996, -0.604808, -0.27681,\n",
       "       0.5360435, 0.060434499999999995, 0.13196549999999999, 0.283603,\n",
       "       -0.601811, 1.013653, -1.025173, -0.183826, 0.298595,\n",
       "       0.48268449999999996, -0.191444, -0.403702, 1.3462494999999999,\n",
       "       0.6377155, -0.15807649999999998, 0.3716465, -0.6163489999999999,\n",
       "       -0.008360000000000001, -0.022057, 0.06511549999999999, -0.856835,\n",
       "       -0.017622, -0.139436, 0.46645349999999997, -0.7970615,\n",
       "       -0.0077009999999999995, -0.566213, -0.029214999999999998, 0.747813,\n",
       "       -0.6417149999999999, 0.509096, 0.146542, 0.2733105,\n",
       "       -0.26309099999999996, -0.2695355, -0.296828, 0.158164, -0.1800775,\n",
       "       1.107935, 0.0806045, -0.3520045, -0.23479699999999998,\n",
       "       -0.5253800000000001, -0.2480595, 0.029821, 0.41248599999999996,\n",
       "       0.01709, 0.5797375, 0.000576, 0.014060000000000001, 0.355101,\n",
       "       -0.0965, -0.1467235, -0.19226549999999998, -0.33634949999999997,\n",
       "       -0.387542, 0.40069899999999997, -0.1473985, 0.187074, -0.542092,\n",
       "       0.365749, -0.4215215, 0.559731, 0.0843565, 0.1259635, 0.645376,\n",
       "       -0.5373555, 0.1619975, 0.9175215, -0.2241185, -0.4406775,\n",
       "       -0.568391, -0.1112705, 0.082465, -0.48430249999999997, -0.1062745,\n",
       "       -0.48071349999999996, 0.030997, 0.4783655, -0.5799995, -0.013828,\n",
       "       0.026290499999999998, -0.526176, -0.1718495, -0.098911, 0.914661,\n",
       "       -0.019813, -0.5824079999999999, 0.589117, -0.1839915, -0.090512,\n",
       "       0.9910264999999999, 0.31275749999999997, -0.450018, -0.368151,\n",
       "       -3.77377, 0.0066605, -0.08171049999999999, 0.202012, 0.4446825,\n",
       "       -0.4048595, 0.7923985, -0.23643199999999998, -0.324287, 0.826769,\n",
       "       -0.1956955, 0.203961, -0.115953, -0.47250499999999995, 0.821125,\n",
       "       -0.264795, 0.8468104999999999, -1.1130825, -0.24348099999999998,\n",
       "       0.4989635, 0.2002725, -0.394258, 0.09866749999999999, -0.1268355,\n",
       "       -0.007186, -0.5617595, -0.009708, -0.5564089999999999,\n",
       "       -0.11591749999999999, -0.488075, -0.3254745, 0.027764499999999998,\n",
       "       -0.36693549999999997, 0.571168, 0.2758135, 0.3972515, 0.514586,\n",
       "       -0.696854, -0.550316, -0.545, -0.018543499999999997, -0.490892,\n",
       "       0.399236, -0.015085999999999999, 0.27308499999999997,\n",
       "       0.25872249999999997, -0.1589885, -0.594683, 0.0864105, 0.400696,\n",
       "       -0.2679775, 0.7412989999999999, -0.2495315, -0.14440799999999998,\n",
       "       -0.22463149999999998, 0.469558, 0.4387895, -0.41217349999999997,\n",
       "       -1.060052, -0.3549245, -0.3894415, -0.9034085,\n",
       "       -0.26324349999999996, -0.022827999999999998, 0.10146899999999999,\n",
       "       -0.086265, -0.243868, -0.352103, -0.455422, -0.218246,\n",
       "       -0.7552979999999999, -0.0551645, 0.971857, -0.498173, -0.291491,\n",
       "       -1.1593965, 0.7275455, 0.476651, -0.45117199999999996, -0.3530105,\n",
       "       0.7457965, -0.3430435, 0.3059735, 0.753122, -0.4746895, 0.244757,\n",
       "       0.2033495, 0.062764, -0.1755795, -0.1414745, 0.46783399999999997,\n",
       "       0.000891, 0.14006949999999999, 0.2773645, 0.16157749999999999,\n",
       "       0.593857, 0.3205515, -0.6671475, 0.0238905, -0.068089,\n",
       "       0.31217849999999997, -0.7205585, -0.27556349999999996, 0.085051,\n",
       "       -0.5614615, -0.45572199999999996, -0.959268, -0.0647935, 0.215999,\n",
       "       -0.4101495, 0.14207699999999998, -0.5889234999999999,\n",
       "       -0.35506000000000004, -0.107833, -0.406831, -0.218586, -0.277448,\n",
       "       1.2360765, 0.26579349999999996, -0.027841499999999998, 0.4326305,\n",
       "       -0.169566, -1.066448, -0.2901795, -0.0489225, 0.160656, 0.5009865,\n",
       "       0.6352679999999999, 0.4875485, 0.3021775, -1.138039, -0.613192,\n",
       "       -0.15631799999999998, -0.128747, 0.19307249999999998, 0.001797,\n",
       "       -0.7106469999999999, -1.2743985, -0.9931245, 0.005004, 0.6700875,\n",
       "       -0.0574215, 0.6000445, 0.6954049999999999, 1.344631, 0.554904,\n",
       "       0.504369, 0.2957305, -0.32356599999999996, 0.648797, -0.2675295,\n",
       "       -0.2469205, -0.345632, -0.8316325, 0.44201399999999996, 0.3351825,\n",
       "       0.7160369999999999, 0.078625, 0.2143125, -0.523805, 0.172491,\n",
       "       0.0060349999999999996, 0.45747499999999997, 0.11119549999999999,\n",
       "       -0.7202179999999999, -0.2039135, -0.373181, 0.1439855, -0.4849635,\n",
       "       0.4234865, -0.3442095, 0.31388299999999997, 0.2408515, 0.531181,\n",
       "       -0.065801, 0.43814149999999996, 0.09224049999999999, 0.0333745,\n",
       "       -0.727646, 0.444162, -0.435556, 0.4115715, -0.042679499999999995,\n",
       "       0.7778634999999999, 0.3389695, -0.27312349999999996, -0.4398085,\n",
       "       -0.280784, -0.06974749999999999, 0.69825, 0.649741,\n",
       "       0.33721599999999996, 0.119417, 0.431122, 0.008296999999999999,\n",
       "       -0.6791444999999999, 0.733687, -0.9725615, -0.034936999999999996,\n",
       "       0.399843, -0.11315399999999999, -0.012718, -0.0022335, 0.5135485,\n",
       "       0.013300999999999999, 0.146125, -0.13078299999999998, 0.180672,\n",
       "       0.0479305, -0.0575295, -0.37743699999999997, -0.443627,\n",
       "       0.5382804999999999, 0.2639705, 0.294607, -0.49166899999999997,\n",
       "       -0.2254755, -0.2948515, 0.0450485, 0.24907449999999998, -0.1432405,\n",
       "       -0.113691, -0.331957, 0.18169749999999998, -0.4364745, 1.0451165,\n",
       "       -0.293591, -0.08569149999999999, 0.0839695, 0.4750915, 0.024797,\n",
       "       -1.9384175, -0.12095399999999999, -0.40295149999999996, 0.4084275,\n",
       "       -0.540226, -0.1594615, -0.0757535, 0.574282, -0.02442, 0.2110495,\n",
       "       0.6475825, -0.795484, -0.9207515, -0.771902, -0.4301675,\n",
       "       -0.3478525, -0.005559499999999999, 0.0825105, 0.023778, 0.52046,\n",
       "       -0.6018600000000001, 0.6324139999999999, -0.3891265, 0.3578475,\n",
       "       -0.029748999999999998, -0.39375299999999996, -0.5701849999999999,\n",
       "       0.1866505, -0.548647, 0.12954849999999998, -0.913812, -1.0573675,\n",
       "       -0.06278399999999999, 0.22768000000000002, 0.337574,\n",
       "       -0.11369349999999999, 0.2803835, 0.25115899999999997,\n",
       "       -0.7976019999999999, -0.49231, -1.291976, 0.06787649999999999,\n",
       "       0.0629825, -0.799651, -0.086742, -0.505548, 0.304854, 0.155303,\n",
       "       -0.18467699999999998, 0.21621549999999998, 0.0510225,\n",
       "       -0.6408929999999999, -0.057470999999999994, 0.5329505, 0.1408305,\n",
       "       0.000263, 0.3302095, 0.4266145, 0.3288605, 0.054813,\n",
       "       -0.12950599999999998, -0.23492149999999998, 0.5571335, -0.329459,\n",
       "       0.166052, 1.1682605, 0.3536555, 0.234489, 0.35691049999999996,\n",
       "       -0.37137349999999997, -0.18624649999999998, 0.23379899999999998,\n",
       "       -0.37612249999999997, -0.109789, 0.22844699999999998, -0.2923205,\n",
       "       -0.946912, 0.08553000000000001, 0.8456115, -0.06819499999999999,\n",
       "       0.1180145, 0.666902, -0.5759704999999999, -0.7108975, -1.0944835,\n",
       "       0.240636, -0.01077, -0.4048195, 0.2529005, -0.173585,\n",
       "       -0.23398249999999998, -0.2094705, 0.17660299999999998, 0.2480365,\n",
       "       0.6779755, -0.1948585, 0.299741, -0.06431, 1.0388195, -0.353271,\n",
       "       -0.41526749999999996, 0.2383295, -0.1990555, -0.0316785,\n",
       "       -0.1217475, -0.148111, -0.2748235, 0.008281499999999999, -0.110955,\n",
       "       0.10896249999999999, -0.282582, -1.175511, 0.5161735,\n",
       "       0.6635519999999999, 0.24227549999999998, 0.092029, -0.7101265,\n",
       "       1.465836, 0.4111255, 0.467248, -0.1201985, 0.31022099999999997,\n",
       "       0.09544749999999999, 0.171783, -0.5730259999999999, -0.435558,\n",
       "       -0.031654, -0.3810785, 0.8638254999999999, 0.151523, -0.1436285,\n",
       "       0.5738369999999999, -0.0535425, -0.2705905, -0.3820455, 0.2359375,\n",
       "       -0.37982000000000005, -0.0080645, 0.048896499999999996, 0.3780305,\n",
       "       -0.170271, -0.3779905, -0.465186, 0.7344445, -0.414145, -0.0681745,\n",
       "       0.288948, -0.30940599999999996, -0.26646749999999997, 0.0243245,\n",
       "       0.285969, -0.2189015, -0.1626075, -0.1169755, 0.2060825,\n",
       "       0.029165499999999997, 0.019300499999999998, -0.30975549999999996,\n",
       "       -0.35716549999999997, 0.324851, 0.240857, -0.2510865, -0.1419375,\n",
       "       -0.119818, -0.025738, 0.418242, -0.07052599999999999, 1.076651,\n",
       "       0.1070685, 0.43646949999999995, -0.3772435, -0.8575395, 0.183723,\n",
       "       0.1997255, 0.341296, -0.2917495, 0.6973925, 0.216493, -0.0798685,\n",
       "       -0.16725399999999999, 0.263069, -0.31377849999999996, -0.4768555,\n",
       "       -0.759915, 0.2108285, 0.5435154999999999, 0.10330299999999999,\n",
       "       0.40100749999999996, -0.117448, -0.6182835, -0.3091055,\n",
       "       0.49082349999999997, 0.5517535, 0.17611849999999998, 1.1565985,\n",
       "       0.20045249999999998, -0.22034399999999998, -0.8394785,\n",
       "       0.16135149999999998, 0.022713, 0.560962, 0.24686149999999998,\n",
       "       0.6986600000000001, -0.049385, -0.3463345, -0.7231314999999999,\n",
       "       0.343783, 0.026320000000000003, 0.452735, 0.12505249999999998,\n",
       "       -0.724235, 0.113948, -0.191576, 0.308347, 0.432515,\n",
       "       0.08814749999999999, 0.48159199999999996, 0.0146225, -0.0042445,\n",
       "       -0.27857699999999996, 0.538125, -0.4809465, -0.6086889999999999,\n",
       "       0.716456, -0.6939204999999999, -0.264056, -0.4009815, 0.231344,\n",
       "       -0.16791399999999998], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class End2End_NCR():\n",
    "    \n",
    "    def __init__(self, word_input_shape): \n",
    "        \n",
    "        self.word_input_shape = word_input_shape\n",
    "        self.hidden_dim   = 150\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        A, B, P = Input((self.word_input_shape,)), Input((self.word_input_shape,)), Input((self.word_input_shape,))\n",
    "        inputs = [A, B, P]\n",
    "\n",
    "        \n",
    "        self.ffnn = Sequential([Dense(self.hidden_dim, use_bias=True),\n",
    "                                     Activation('relu'),\n",
    "                                     Dropout(rate=0.2, seed = 7),\n",
    "                                     Dense(1, activation='linear')])\n",
    "\n",
    "        PA = Multiply()([inputs[0], inputs[2]])\n",
    "        PB = Multiply()([inputs[1], inputs[2]])\n",
    "\n",
    "        PA = Concatenate(axis=-1)([P, A, PA])\n",
    "        PB = Concatenate(axis=-1)([P, B, PB])\n",
    "        PA_score = self.ffnn(PA)\n",
    "        PB_score = self.ffnn(PB)\n",
    "        # Fix the Neither to score 0.\n",
    "        score_e  = Lambda(lambda x: K.zeros_like(x))(PB_score)\n",
    "        \n",
    "        #Final Output\n",
    "        output = Concatenate(axis=-1)([PA_score, PB_score, score_e])\n",
    "        output = Activation('softmax')(output)        \n",
    "        model = Model(inputs, output)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1024)         0           input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1024)         0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3072)         0           input_3[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3072)         0           input_3[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            461101      concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 3)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 3)            0           concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 461,101\n",
      "Trainable params: 461,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "End2End_NCR(1024).build().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read development embeddings from json file - this is the output of Bert\n",
    "development = pd.read_json('data/large-emb-gap-development.json')\n",
    "X_development, Y_development = parse_json(development)\n",
    "\n",
    "validation = pd.read_json('data/large-emb-gap-validation.json')\n",
    "X_validation, Y_validation = parse_json(validation)\n",
    "\n",
    "test = pd.read_json('data/large-emb-gap-test.json')\n",
    "X_test, Y_test = parse_json(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_A</th>\n",
       "      <th>emb_B</th>\n",
       "      <th>emb_P</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.40596499999999996, -0.4736605, -0.12929649...</td>\n",
       "      <td>[-0.453505, -0.7266819999999999, -0.597712, 0....</td>\n",
       "      <td>[-0.61323, -0.160519, 0.481929, -0.55641299999...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.514829, -0.6953269999999999, 0.278206, 0.0...</td>\n",
       "      <td>[-0.704917, 0.042984999999999995, 0.1404515, -...</td>\n",
       "      <td>[-0.593152, -0.178061, 0.194238, -0.5945929999...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.6475200000000001, -0.018141, -0.00232, 0.2...</td>\n",
       "      <td>[-0.65946025, -0.52119425, 0.16222825, 0.00214...</td>\n",
       "      <td>[0.264017, -0.798705, -0.11953899999999999, -0...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.5062059999999999, -0.06980599999999999, 0.1...</td>\n",
       "      <td>[-0.1498205, -0.140093, 0.5434215, -0.108469, ...</td>\n",
       "      <td>[-0.057193999999999995, -0.37878599999999996, ...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.018103499999999998, -0.08592325, 0.39331325...</td>\n",
       "      <td>[-0.578073, -0.044946, 0.35519, 0.028821, -0.2...</td>\n",
       "      <td>[-0.6689729999999999, -0.380459, -0.269386, 0....</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               emb_A  \\\n",
       "0  [-0.40596499999999996, -0.4736605, -0.12929649...   \n",
       "1  [-0.514829, -0.6953269999999999, 0.278206, 0.0...   \n",
       "2  [-0.6475200000000001, -0.018141, -0.00232, 0.2...   \n",
       "3  [0.5062059999999999, -0.06980599999999999, 0.1...   \n",
       "4  [0.018103499999999998, -0.08592325, 0.39331325...   \n",
       "\n",
       "                                               emb_B  \\\n",
       "0  [-0.453505, -0.7266819999999999, -0.597712, 0....   \n",
       "1  [-0.704917, 0.042984999999999995, 0.1404515, -...   \n",
       "2  [-0.65946025, -0.52119425, 0.16222825, 0.00214...   \n",
       "3  [-0.1498205, -0.140093, 0.5434215, -0.108469, ...   \n",
       "4  [-0.578073, -0.044946, 0.35519, 0.028821, -0.2...   \n",
       "\n",
       "                                               emb_P label  \n",
       "0  [-0.61323, -0.160519, 0.481929, -0.55641299999...     A  \n",
       "1  [-0.593152, -0.178061, 0.194238, -0.5945929999...     A  \n",
       "2  [0.264017, -0.798705, -0.11953899999999999, -0...     B  \n",
       "3  [-0.057193999999999995, -0.37878599999999996, ...     B  \n",
       "4  [-0.6689729999999999, -0.380459, -0.269386, 0....     B  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "development.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_development.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_development.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454, 3072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lgbm_large_mgap = pd.read_csv('output/lgbm_large_mgap.csv', index_col='ID')\n",
    "lgbm_base_mgap = pd.read_csv('output/lgbm_base_mgap.csv', index_col='ID')\n",
    "\n",
    "fname = lgbm_large_mgap.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [(lgbm_large_mgap, 3), (lgbm_base_mgap, 1)]\n",
    "res_prob = np.zeros_like(lgbm_large_mgap.values)\n",
    "total_weight = 0\n",
    "for dfx, w in weights:\n",
    "    res_prob += lgbm_large_mgap.values*w\n",
    "    total_weight += w\n",
    "res_prob /= total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(res_prob, columns=['A', 'B', 'NEITHER'])\n",
    "res['ID'] = fname\n",
    "res.to_csv(\"output/blend.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
